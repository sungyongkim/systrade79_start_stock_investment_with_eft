#!/usr/bin/env python3
"""
Transfer stock data from Firestore to BigQuery
"""

import pandas as pd
from google.cloud import firestore
from google.cloud import bigquery
from google.oauth2 import service_account
from datetime import datetime
import time

# í™˜ê²½ ë³€ìˆ˜
GOOGLE_SERVICE_ACCOUNT = "/Users/cg01-piwoo/my_quant/access_info/data/quantsungyong-663604552de9.json"
FIRESTORE_DATABASE_ID = "quant-data-etl"
FIRESTORE_COLLECTION = "stock_raw_data"
BIGQUERY_DATASET = "finviz_data"
BIGQUERY_TABLE = "stock_data"

# ì²˜ë¦¬í•  í‹°ì»¤ ëª©ë¡
tickers = [
    "AGQ", "AMZN", "BTAL", "EFNL", "ENIC", "ERX", "ERY", "EWG", "EWH", "EWO", "EWS",
    "FAS", "FAZ", "FEUZ", "FGM", "FLGR", "FXY", "GOOGL", "IDOG", "IVLU", "IWM", "KOLD", 
    "KSTR", "KTEC", "LDEM", "META", "MSFT", "NFLX", "NVDA", "ONEQ", "ROM", "RXD", "RXL", 
    "SCO", "SHLD", "SOXL", "SOXS", "SQQQ", "TAIL", "TBF", "TBT", "TBX", "TECL", "TECS", 
    "TQQQ", "TSLA", "TZA", "UBT", "UCC", "UCO", "UGE", "UGL", "UJB", "UNG", "URE", "UST", 
    "UXI", "UYG", "UYM", "VDE", "VXX", "XLB", "XLC", "XLE", "XLF", "XLI", "XLK", "XLP",
    "XLRE", "XLV", "XPP", "YCS", "YINN", "ZSL"
]

def get_credentials():
    """ì¸ì¦ ì •ë³´ ìƒì„±"""
    return service_account.Credentials.from_service_account_file(
        GOOGLE_SERVICE_ACCOUNT,
        scopes=[
            "https://www.googleapis.com/auth/bigquery",
            "https://www.googleapis.com/auth/datastore"
        ]
    )

def get_firestore_data(ticker, credentials):
    """Firestoreì—ì„œ í‹°ì»¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        # Firestore í´ë¼ì´ì–¸íŠ¸
        db = firestore.Client(
            project=credentials.project_id,
            credentials=credentials,
            database=FIRESTORE_DATABASE_ID
        )
        
        # ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        doc_ref = db.collection(FIRESTORE_COLLECTION).document(ticker)
        doc = doc_ref.get()
        
        if not doc.exists:
            print(f"âŒ {ticker}: Firestoreì— ë°ì´í„° ì—†ìŒ")
            return None
            
        data = doc.to_dict()
        
        if 'data' not in data:
            print(f"âŒ {ticker}: ê°€ê²© ë°ì´í„° ì—†ìŒ")
            return None
            
        price_data = data['data']
        
        # DataFrame ìƒì„±
        df = pd.DataFrame({
            'ticker': ticker,
            'date': pd.to_datetime(price_data['dates']),
            'open': price_data['open'],
            'high': price_data['high'],
            'low': price_data['low'],
            'close': price_data['close'],
            'volume': price_data['volume']
        })
        
        return df
        
    except Exception as e:
        print(f"âŒ {ticker}: Firestore ì½ê¸° ì˜¤ë¥˜ - {str(e)}")
        return None

def create_bigquery_table_if_not_exists(client):
    """BigQuery í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±"""
    dataset_ref = client.dataset(BIGQUERY_DATASET)
    table_ref = dataset_ref.table(BIGQUERY_TABLE)
    
    try:
        client.get_table(table_ref)
        print(f"âœ… í…Œì´ë¸” {BIGQUERY_DATASET}.{BIGQUERY_TABLE}ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    except:
        print(f"ğŸ“¦ í…Œì´ë¸” {BIGQUERY_DATASET}.{BIGQUERY_TABLE}ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        schema = [
            bigquery.SchemaField("ticker", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("date", "DATE", mode="REQUIRED"),
            bigquery.SchemaField("open", "FLOAT64"),
            bigquery.SchemaField("high", "FLOAT64"),
            bigquery.SchemaField("low", "FLOAT64"),
            bigquery.SchemaField("close", "FLOAT64"),
            bigquery.SchemaField("volume", "INT64"),
        ]
        table = bigquery.Table(table_ref, schema=schema)
        table = client.create_table(table)
        print(f"âœ… í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

def upload_to_bigquery(df, ticker, client):
    """DataFrameì„ BigQueryì— ì—…ë¡œë“œ"""
    table_id = f"{client.project}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}"
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (í•´ë‹¹ í‹°ì»¤ë§Œ)
    delete_query = f"""
    DELETE FROM `{table_id}`
    WHERE ticker = '{ticker}'
    """
    
    try:
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        delete_job = client.query(delete_query)
        delete_job.result()
        
        # ìƒˆ ë°ì´í„° ì—…ë¡œë“œ
        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_APPEND",
            schema=[
                bigquery.SchemaField("ticker", "STRING"),
                bigquery.SchemaField("date", "DATE"),
                bigquery.SchemaField("open", "FLOAT64"),
                bigquery.SchemaField("high", "FLOAT64"),
                bigquery.SchemaField("low", "FLOAT64"),
                bigquery.SchemaField("close", "FLOAT64"),
                bigquery.SchemaField("volume", "INT64"),
            ]
        )
        
        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
        job.result()
        
        return True
    except Exception as e:
        print(f"âŒ BigQuery ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print(f"Firestore â†’ BigQuery ë°ì´í„° ì „ì†¡ ì‹œì‘")
    print(f"ì´ {len(tickers)}ê°œ í‹°ì»¤ ì²˜ë¦¬")
    print("=" * 60)
    
    # ì¸ì¦ ì •ë³´
    credentials = get_credentials()
    
    # BigQuery í´ë¼ì´ì–¸íŠ¸
    bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)
    
    # í…Œì´ë¸” ìƒì„± í™•ì¸
    create_bigquery_table_if_not_exists(bq_client)
    
    # ì²˜ë¦¬ í†µê³„
    success_count = 0
    failed_tickers = []
    total_records = 0
    
    # ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 5
    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        
        print(f"\në°°ì¹˜ #{batch_num} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ)")
        print(f"í‹°ì»¤: {', '.join(batch)}")
        
        for ticker in batch:
            # Firestoreì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            df = get_firestore_data(ticker, credentials)
            
            if df is not None and not df.empty:
                # BigQueryì— ì—…ë¡œë“œ
                if upload_to_bigquery(df, ticker, bq_client):
                    success_count += 1
                    total_records += len(df)
                    print(f"âœ… {ticker}: {len(df)}ê°œ ë ˆì½”ë“œ ì—…ë¡œë“œ ì™„ë£Œ")
                else:
                    failed_tickers.append(ticker)
            else:
                failed_tickers.append(ticker)
        
        # ë‹¤ìŒ ë°°ì¹˜ ì „ ì ì‹œ ëŒ€ê¸°
        if i + batch_size < len(tickers):
            time.sleep(2)
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ ì „ì†¡ ì™„ë£Œ!")
    print(f"ì„±ê³µ: {success_count}ê°œ í‹°ì»¤")
    print(f"ì‹¤íŒ¨: {len(failed_tickers)}ê°œ í‹°ì»¤")
    print(f"ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")
    
    if failed_tickers:
        print(f"\nì‹¤íŒ¨í•œ í‹°ì»¤: {', '.join(failed_tickers)}")
    
    # ë°ì´í„° í™•ì¸ ì¿¼ë¦¬
    print("\nğŸ“Š ë°ì´í„° í™•ì¸:")
    check_query = f"""
    SELECT 
        COUNT(DISTINCT ticker) as unique_tickers,
        COUNT(*) as total_records,
        MIN(date) as earliest_date,
        MAX(date) as latest_date
    FROM `{bq_client.project}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}`
    WHERE ticker IN ({','.join([f"'{t}'" for t in tickers])})
    """
    
    results = bq_client.query(check_query).result()
    for row in results:
        print(f"  - í‹°ì»¤ ìˆ˜: {row.unique_tickers}")
        print(f"  - ì´ ë ˆì½”ë“œ: {row.total_records:,}")
        print(f"  - ê¸°ê°„: {row.earliest_date} ~ {row.latest_date}")

if __name__ == "__main__":
    main()